# Training (very basic CBOW with numpy)
import numpy as np

# Initialize weights
W1 = np.random.rand(vocab_size, embed_dim)
W2 = np.random.rand(embed_dim, vocab_size)

learning_rate = 0.01
epochs = 1000

for epoch in range(epochs):
    loss = 0
    for context, target in data:
        # Convert words to indices
        context_ids = [word_to_ix[w] for w in context]
        target_id = word_to_ix[target]

        # Forward pass
        x = np.mean(W1[context_ids], axis=0)
        z = np.dot(x, W2)
        y_pred = np.exp(z) / np.sum(np.exp(z))  # Softmax

        # Calculate loss
        loss -= np.log(y_pred[target_id])

        # Backpropagation (gradient updates)
        y_pred[target_id] -= 1  # This is (y_hat - y)

        # Gradient for W2
        dW2 = np.outer(x, y_pred)
        W2 -= learning_rate * dW2

        # Gradient for W1 (only for context words)
        # d_Loss/d_x = W2 @ (y_hat - y)
        grad_hidden = np.dot(W2, y_pred)

        # Each context word embedding contributes to x, so share the gradient
        W1[context_ids] -= learning_rate * grad_hidden / len(context_ids)

    if epoch % 100 == 0:
        # Fixed: Removed extra parenthesis from the original
        print(f'Epoch {epoch}, Loss: {loss:.4f}')

print("Similar words to 'process':")
similarities = np.dot(W1, W1[word_to_ix['process']])
sorted_words = np.argsort(-similarities)

for idx in sorted_words[:5]:
    print(ix_to_word[idx])
